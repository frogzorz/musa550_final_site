[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hello there!\n\n\nMy name is Jamie Song, and I took MUSA 550 in Fall 2024 with Professor Eric Delmelle.\nYou can find more information about me on my LinkedIn page.\nI’m also a research and program coordinator at Penn Trauma.\n\nIf you’re interested in injury research and/or epidemiology, please reach out!",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "analysis/crash-data-download.html",
    "href": "analysis/crash-data-download.html",
    "title": "Loading the data",
    "section": "",
    "text": "First, we will load our data from the following sources:\n# load necessary libraries\nimport os\nimport requests\nimport zipfile\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport contextily as ctx\nimport folium\nimport json\nfrom IPython.display import display, HTML\nfrom folium.plugins import TimeSliderChoropleth\nfrom shapely.ops import nearest_points\n# define download paths\nbase_url = \"https://gis.penndot.gov/gishub/crashZip/County/Philadelphia/Philadelphia_\"\ndownload_dir = \"philly_crash_data\"\nos.makedirs(download_dir, exist_ok=True)\n\n# create an empty list to store dataframes\ncrash_data_list = []\n\n# loop through 2018 to 2023\nfor year in range(2018, 2024):\n    print(f\"Processing data for year {year}...\")\n    zip_filename = f\"{download_dir}/Philadelphia_{year}.zip\"\n    csv_filename = f\"{download_dir}/CRASH_PHILADELPHIA_{year}.csv\"\n\n    # download the zip file if it doesn't exist yet\n    if not os.path.exists(zip_filename):\n        url = f\"{base_url}{year}.zip\"\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(zip_filename, \"wb\") as file:\n                file.write(response.content)\n            print(f\"Downloaded: {zip_filename}\")\n        else:\n            print(f\"Failed to download data for year {year}. URL: {url}\")\n            continue\n\n    # extract the csv\n    if not os.path.exists(csv_filename):\n        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n            csv_files = [name for name in zip_ref.namelist() if name.endswith(f\"CRASH_PHILADELPHIA_{year}.csv\")]\n            if csv_files:\n                zip_ref.extract(csv_files[0], download_dir)\n                print(f\"Extracted: {csv_filename}\")\n            else:\n                print(f\"Relevant CSV file not found in {zip_filename}\")\n                continue\n\n    # load csv as df\n    if os.path.exists(csv_filename):\n        df = pd.read_csv(csv_filename)\n        df['year'] = year\n        crash_data_list.append(df)\n\n# combine all years into a single df\nif crash_data_list:\n    all_crashes = pd.concat(crash_data_list, ignore_index=True)\n    print(\"Combined all years into a single DataFrame.\")\nelse:\n    print(\"No data loaded.\")\n\n# delete the zip and csv files\nfor year in range(2018, 2024):\n    zip_filename = f\"{download_dir}/Philadelphia_{year}.zip\"\n    csv_filename = f\"{download_dir}/CRASH_PHILADELPHIA_{year}.csv\"\n    \n    if os.path.exists(zip_filename):\n        os.remove(zip_filename)\n        print(f\"Deleted: {zip_filename}\")\n    \n    if os.path.exists(csv_filename):\n        os.remove(csv_filename)\n        print(f\"Deleted: {csv_filename}\")\n\nProcessing data for year 2018...\nProcessing data for year 2019...\nProcessing data for year 2020...\nProcessing data for year 2021...\nProcessing data for year 2022...\nProcessing data for year 2023...\nCombined all years into a single DataFrame.\nDeleted: philly_crash_data/Philadelphia_2018.zip\nDeleted: philly_crash_data/CRASH_PHILADELPHIA_2018.csv\nDeleted: philly_crash_data/Philadelphia_2019.zip\nDeleted: philly_crash_data/CRASH_PHILADELPHIA_2019.csv\nDeleted: philly_crash_data/Philadelphia_2020.zip\nDeleted: philly_crash_data/CRASH_PHILADELPHIA_2020.csv\nDeleted: philly_crash_data/Philadelphia_2021.zip\nDeleted: philly_crash_data/CRASH_PHILADELPHIA_2021.csv\nDeleted: philly_crash_data/Philadelphia_2022.zip\nDeleted: philly_crash_data/CRASH_PHILADELPHIA_2022.csv\nDeleted: philly_crash_data/Philadelphia_2023.zip\nDeleted: philly_crash_data/CRASH_PHILADELPHIA_2023.csv\n# select for non-motorist crashes only\nnon_motorist_crashes = all_crashes[all_crashes['NONMOTR_COUNT'] &gt; 0]\n\n# filter out records with missing geography\nnon_motorist_crashes = non_motorist_crashes.dropna(subset=['DEC_LAT', 'DEC_LONG'])\n\n# convert to gdf using lat and long\ngeometry = gpd.points_from_xy(non_motorist_crashes['DEC_LONG'], non_motorist_crashes['DEC_LAT'])\nnon_motorist_gdf = gpd.GeoDataFrame(non_motorist_crashes, geometry=geometry, crs=\"EPSG:4326\")",
    "crumbs": [
      "Project introduction",
      "Loading the data"
    ]
  },
  {
    "objectID": "analysis/crash-data-download.html#visualizing-the-crash-data",
    "href": "analysis/crash-data-download.html#visualizing-the-crash-data",
    "title": "Loading the data",
    "section": "Visualizing the crash data",
    "text": "Visualizing the crash data\n\nCrash point data by year\nNow that we have imported and prepared the crash data, we will quickly visualize them in the following interactive map.\nYears can be toggled on and off to see distributions of crashes across time. Reducing the number of years visualized as well as zooming into the map reduces the relative size of the dots, making it easier to see the spatial distribution of crashes.\n\n# create a base map centered on Philadelphia\nm = folium.Map(location=[39.9526, -75.1652], zoom_start=12, tiles=\"cartodb positron\")\n\n# group crashes by year\nyear_groups = non_motorist_gdf.groupby('year')\n\n# create a feature group for each year\nyear_layers = {}\n\nfor year, group in year_groups:\n    year_data = group.copy()\n    \n    # create geometries from lat and long\n    year_data['geometry'] = gpd.GeoSeries.from_xy(year_data['DEC_LONG'], year_data['DEC_LAT'])\n    year_gdf = gpd.GeoDataFrame(year_data, geometry='geometry', crs=\"EPSG:4326\")\n    \n    # create a feature group for year\n    year_layer = folium.FeatureGroup(name=str(year))\n    \n    for _, row in year_gdf.iterrows():\n        folium.CircleMarker(\n            location=[row['DEC_LAT'], row['DEC_LONG']],\n            radius=5,\n            popup=f\"Crash ID: {row['CRN']}&lt;br&gt;Year: {row['year']}\",\n            color='blue',\n            fill=True,\n            fill_color='blue',\n            fill_opacity=0.6\n        ).add_to(year_layer)\n    \n    # add year layer to dictionary\n    year_layers[year] = year_layer\n\n# add all layers to map\nfor year, layer in year_layers.items():\n    layer.add_to(m)\n\n# add layer control to toggle years with checkboxes\nfolium.LayerControl(collapsed=False).add_to(m)\n\n# display map\nm\n\n\n# save non motorist crashes as a geojson file for next notebook\nnon_motorist_gdf.to_file(\"non_motorist_gdf.geojson\", driver=\"GeoJSON\")",
    "crumbs": [
      "Project introduction",
      "Loading the data"
    ]
  },
  {
    "objectID": "analysis/data-processing.html",
    "href": "analysis/data-processing.html",
    "title": "Further data processing and viz.",
    "section": "",
    "text": "# install packages if needed\n#%pip install libpysal\n#%pip install esda\n#%pip install census\n\nimport os\nimport requests\nimport zipfile\nimport pysal\nimport esda\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport contextily as ctx\nimport folium\nimport cenpy as c\nimport osmnx as ox\nimport seaborn as sns\nfrom IPython.display import display, HTML\nfrom folium.plugins import TimeSliderChoropleth\nfrom census import Census\nfrom us import states\nfrom sklearn.preprocessing import QuantileTransformer\nfrom shapely.geometry import Point, LineString\nfrom esda.moran import Moran_Local\nfrom libpysal.weights import Queen\nfrom shapely.ops import nearest_points",
    "crumbs": [
      "Project introduction",
      "Further data processing and viz."
    ]
  },
  {
    "objectID": "analysis/data-processing.html#visualizing-maps-of-crashes-per-census-tract",
    "href": "analysis/data-processing.html#visualizing-maps-of-crashes-per-census-tract",
    "title": "Further data processing and viz.",
    "section": "Visualizing maps of crashes per census tract",
    "text": "Visualizing maps of crashes per census tract\nWe will now visualize maps of the two crashes per tract metrics.\n\nAll crashes by census tract, normalized by land area (sq. km.)\n\nm_land  # display crashes per land area\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nAll crashes by census tract, normalized by road length (km)\n\nm_road  # display crashes per road km\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nBy comparing the two maps, we can see that the spatial distribution of the two metrics are quite similar, with the greatest burden of crashes in Center City, Kensington, and West Philadelphia, as well as along Broad Street, an important north-south arterial road.",
    "crumbs": [
      "Project introduction",
      "Further data processing and viz."
    ]
  },
  {
    "objectID": "analysis/data-processing.html#further-data-processing-and-collection",
    "href": "analysis/data-processing.html#further-data-processing-and-collection",
    "title": "Further data processing and viz.",
    "section": "Further data processing and collection",
    "text": "Further data processing and collection\nWe will process our data further by calculating the proportion of tertiary and residential roads in each census tract.\n\n# select residential and tertiary roads\nresidential_edges = edges[edges[\"highway\"] == \"residential\"]\ntertiary_edges = edges[edges[\"highway\"] == \"tertiary\"]\n\n# ensure geo_data is in the same CRS as edges\ngeo_data = geo_data.to_crs(edges.crs)\n\n# spatial join all roads with census tracts\nedges_with_tract = gpd.sjoin(edges, geo_data, how=\"inner\", predicate=\"intersects\")\n\n# spatial join only residential roads with census tracts\nresidential_edges_with_tract = gpd.sjoin(residential_edges, geo_data, how=\"inner\", predicate=\"intersects\")\n\n# spatial join only tertiary roads with census tracts\ntertiary_edges_with_tract = gpd.sjoin(tertiary_edges, geo_data, how=\"inner\", predicate=\"intersects\")\n\n# summarize total road length per tract\ntotal_length_by_tract = (\n    edges_with_tract.groupby(\"GEOID10\")[\"length\"]\n    .sum()\n    .reset_index(name=\"total_length\")\n)\n\n# total residential road length per tract\nresidential_length_by_tract = (\n    residential_edges_with_tract.groupby(\"GEOID10\")[\"length\"]\n    .sum()\n    .reset_index(name=\"residential_length\")\n)\n\n# total tertiary road length per tract\ntertiary_length_by_tract = (\n    tertiary_edges_with_tract.groupby(\"GEOID10\")[\"length\"]\n    .sum()\n    .reset_index(name=\"tertiary_length\")\n)\n\n# merge total lengths, residential lengths and merge tertiary lengths\ngeo_data = geo_data.merge(total_length_by_tract, on=\"GEOID10\", how=\"left\")\ngeo_data = geo_data.merge(residential_length_by_tract, on=\"GEOID10\", how=\"left\")\ngeo_data = geo_data.merge(tertiary_length_by_tract, on=\"GEOID10\", how=\"left\")\n\n# calculate the proportion of residential roads\ngeo_data[\"residential_proportion\"] = geo_data[\"residential_length\"] / geo_data[\"total_length\"]\n\n# calculate the proportion of tertiary roads\ngeo_data[\"tertiary_proportion\"] = geo_data[\"tertiary_length\"] / geo_data[\"total_length\"]\n\n# handle missing values\ngeo_data[\"residential_proportion\"] = geo_data[\"residential_proportion\"].fillna(0)\ngeo_data[\"tertiary_proportion\"] = geo_data[\"tertiary_proportion\"].fillna(0)\n\n\nACS data\nWe will also collect data from the ACS 2022 5-year estimates on commuting characteristics and vehicle ownership, as well as economic and racial/ethnic demographics. In detail, they are as follows:\n\nCommuting\n\nDrive alone\nCarpool\nPublic transit\nWalking\nBiking\nWork from home\n\nVehicle ownership\n\nNone\nOne\nTwo\nThree or more (excluded from analysis)\n\nEconomics\n\nMedian household income\nPoverty rate\n\nRace/ethnicity\n\nWhite\nBlack\nHispanic/Latinx\nAsian\n\n\n\n# set census API key\nCENSUS_API_KEY = \"811e1f6f8d1f299cb75a0f0c07e01aafd801fa79\"\nc = Census(CENSUS_API_KEY)\n\n# define FIPS codes for Philadelphia\nSTATE_FIPS = states.PA.fips\nCOUNTY_FIPS = \"101\"\n\n# define ACS variables to pull\nacs_variables = {\n    \"comm_total\": \"B08006_001E\",\n    \"commute_drive_alone\": \"B08006_003E\",\n    \"commute_carpool\": \"B08006_004E\",\n    \"commute_public_transport\": \"B08006_008E\",\n    \"commute_walked\": \"B08006_015E\",\n    \"commute_bike\": \"B08006_014E\",\n    \"commute_work_from_home\": \"B08006_017E\",\n    \"veh_total\": \"B25044_001E\",\n    \"vehicle_none_owner\": \"B25044_003E\",\n    \"vehicle_one_owner\": \"B25044_004E\",\n    \"vehicle_two_owner\": \"B25044_005E\",\n    \"vehicle_three_owner\": \"B25044_006E\",\n    \"vehicle_four_owner\": \"B25044_007E\",\n    \"vehicle_five_or_more_owner\": \"B25044_008E\",\n    \"vehicle_none_renter\": \"B25044_010E\",\n    \"vehicle_one_renter\": \"B25044_011E\",\n    \"vehicle_two_renter\": \"B25044_012E\",\n    \"vehicle_three_renter\": \"B25044_013E\",\n    \"vehicle_four_renter\": \"B25044_014E\",\n    \"vehicle_five_or_more_renter\": \"B25044_015E\",\n    \"median_income\": \"B19013_001E\",\n    \"pov_total\": \"B17001_001E\",\n    \"poverty\": \"B17001_002E\",\n    \"white_alone\": \"B02001_002E\",\n    \"black_alone\": \"B02001_003E\",\n    \"asian_alone\": \"B02001_005E\",\n    \"hispanic\": \"B03003_003E\",\n    \"total_population\": \"B02001_001E\"\n}\n\n# fetch ACS data\nacs_data = c.acs5.state_county_tract(\n    fields=list(acs_variables.values()),\n    state_fips=STATE_FIPS,\n    county_fips=COUNTY_FIPS,\n    tract=\"*\",\n    year=2022\n)\n\n# convert ACS data to df\nacs_df = pd.DataFrame(acs_data)\n\n# rename columns for clarity\nacs_df.rename(columns={v: k for k, v in acs_variables.items()}, inplace=True)\n\n# convert median_income values under 1 to 0\nacs_df[\"median_income\"] = acs_df[\"median_income\"].apply(lambda x: 0 if x &lt; 1 else x)\n\n# add three, four, and five or more vehicles variables to get three or more vehicles\nacs_df[\"vehicle_three_or_more_owner\"] = (\n    acs_df[\"vehicle_three_owner\"] +\n    acs_df[\"vehicle_four_owner\"] +\n    acs_df[\"vehicle_five_or_more_owner\"]\n)\nacs_df[\"vehicle_three_or_more_renter\"] = (\n    acs_df[\"vehicle_three_renter\"] +\n    acs_df[\"vehicle_four_renter\"] +\n    acs_df[\"vehicle_five_or_more_renter\"]\n)\n\n# combine owner and renter variables to create totals\nacs_df[\"vehicle_none\"] = (\n    acs_df[\"vehicle_none_owner\"] +\n    acs_df[\"vehicle_none_renter\"]\n)\nacs_df[\"vehicle_one\"] = (\n    acs_df[\"vehicle_one_owner\"] +\n    acs_df[\"vehicle_one_renter\"]\n)\nacs_df[\"vehicle_two\"] = (\n    acs_df[\"vehicle_two_owner\"] +\n    acs_df[\"vehicle_two_renter\"]\n)\nacs_df[\"vehicle_three_or_more\"] = (\n    acs_df[\"vehicle_three_or_more_owner\"] +\n    acs_df[\"vehicle_three_or_more_renter\"]\n)\n\n# delete original variables\nacs_df.drop(columns=[\n    \"vehicle_none_owner\",\n    \"vehicle_one_owner\",\n    \"vehicle_two_owner\",\n    \"vehicle_three_owner\",\n    \"vehicle_four_owner\",\n    \"vehicle_five_or_more_owner\",\n    \"vehicle_none_renter\",\n    \"vehicle_one_renter\",\n    \"vehicle_two_renter\",\n    \"vehicle_three_renter\",\n    \"vehicle_four_renter\",\n    \"vehicle_five_or_more_renter\",\n    \"vehicle_three_or_more_owner\",\n    \"vehicle_three_or_more_renter\"\n], inplace=True)\n\n\n# calculate percentages for demographic variables\nacs_df[\"pct_white\"] = acs_df[\"white_alone\"] / acs_df[\"total_population\"]\nacs_df[\"pct_black\"] = acs_df[\"black_alone\"] / acs_df[\"total_population\"]\nacs_df[\"pct_asian\"] = acs_df[\"asian_alone\"] / acs_df[\"total_population\"]\nacs_df[\"pct_hispanic\"] = acs_df[\"hispanic\"] / acs_df[\"total_population\"]\nacs_df[\"pct_comm_drive_alone\"] = acs_df[\"commute_drive_alone\"] / acs_df[\"comm_total\"]\nacs_df[\"pct_comm_carpool\"] = acs_df[\"commute_carpool\"] / acs_df[\"comm_total\"]\nacs_df[\"pct_comm_transit\"] = acs_df[\"commute_public_transport\"] / acs_df[\"comm_total\"]\nacs_df[\"pct_comm_walk\"] = acs_df[\"commute_walked\"] / acs_df[\"comm_total\"]\nacs_df[\"pct_comm_bike\"] = acs_df[\"commute_bike\"] / acs_df[\"comm_total\"]\nacs_df[\"pct_comm_wfh\"] = acs_df[\"commute_work_from_home\"] / acs_df[\"comm_total\"]\nacs_df[\"pct_vehicle0\"] = acs_df[\"vehicle_none\"] / acs_df[\"veh_total\"]\nacs_df[\"pct_vehicle1\"] = acs_df[\"vehicle_one\"] / acs_df[\"veh_total\"]\nacs_df[\"pct_vehicle2\"] = acs_df[\"vehicle_two\"] / acs_df[\"veh_total\"]\nacs_df[\"pct_vehicle3plus\"] = acs_df[\"vehicle_three_or_more\"] / acs_df[\"veh_total\"]\nacs_df[\"pct_poverty\"] = acs_df[\"poverty\"] / acs_df[\"pov_total\"]\n\n# reconstruct GEOID10 in acs_df\nacs_df[\"GEOID10\"] = (\n    acs_df[\"state\"].astype(str).str.zfill(2) +\n    acs_df[\"county\"].astype(str).str.zfill(3) +\n    acs_df[\"tract\"].astype(str).str.zfill(6)\n)\n\n# merge ACS data into geo_data using GEOID10\ngeo_data_acs = geo_data.merge(acs_df, on=\"GEOID10\", how=\"left\")\n\n# handle missing values\ngeo_data_acs.fillna(0, inplace=True)\n\n# drop unnecessary columns\ngeo_data_acs.drop(columns=[\"state\", \"county\", \"tract\"], inplace=True)\n\n# save geo_data as a geojson file for next notebook\ngeo_data_acs.to_file(\"geo_data_crashes.geojson\", driver=\"GeoJSON\")",
    "crumbs": [
      "Project introduction",
      "Further data processing and viz."
    ]
  },
  {
    "objectID": "analysis/data-processing.html#summarizing-acs-data",
    "href": "analysis/data-processing.html#summarizing-acs-data",
    "title": "Further data processing and viz.",
    "section": "Summarizing ACS data",
    "text": "Summarizing ACS data\nAs with many injury-related phenomena, motor vehicle injuries and non-motorist crashes show disparities in who they affect (NHTSA). We will describe this trend in Philadelphia using the data we pulled from the ACS.\n\nHistograms\nFirst, we will produce a set of histograms to view the distributions of several ACS variables.\n\n# list of demographic variables to plot\nvariables = [\n    'median_income', 'pct_white', 'pct_black', 'pct_hispanic', 'pct_asian',\n    'pct_poverty', 'pct_comm_drive_alone', 'pct_comm_carpool', 'pct_comm_transit',\n    'pct_comm_walk', 'pct_comm_bike'\n]\n\n# create a histogram for each variable\nfor variable in variables:\n    plt.figure(figsize=(10, 6))\n    plt.hist(geo_data_acs[variable], bins=20, edgecolor='black')\n    plt.title(f'Histogram of {variable}')\n    plt.xlabel(variable)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the racial/ethnic demographic variables as well as many of the commute-related variables do not have apparently normal distributions. To minimize the bias from skewed data, we will calculate median instead of mean for these.\n\n\nSummary table\nThe following table summarizes the ACS variables by quintile of crashes by road length (km) by census tract.\n\n# multiply all percentage variables by 100\npercentage_columns = [\n    'pct_white', 'pct_black', 'pct_hispanic', 'pct_asian', 'pct_poverty',\n    'pct_comm_drive_alone', 'pct_comm_carpool', 'pct_comm_transit',\n    'pct_comm_walk', 'pct_comm_bike'\n]\ngeo_data_acs[percentage_columns] = geo_data_acs[percentage_columns] * 100\n\n# group by quintile and calculate the mean demographics for each quintile\nquintile_demographics = geo_data_acs.groupby('quintile_road').agg({\n    'median_income': 'mean',\n    'pct_white': 'median',\n    'pct_black': 'median',\n    'pct_hispanic': 'median',\n    'pct_asian': 'median',\n    'pct_poverty': 'mean',\n    'pct_comm_drive_alone': 'median',\n    'pct_comm_carpool': 'median',\n    'pct_comm_transit': 'median',\n    'pct_comm_walk': 'median',\n    'pct_comm_bike': 'median'\n}).reset_index()\n\n\n# rename columns to include the appropriate metric\nquintile_demographics.rename(columns={\n    'median_income': 'median income (mean)',\n    'pct_white': 'pct white (median)',\n    'pct_black': 'pct black (median)',\n    'pct_hispanic': 'pct hispanic (median)',\n    'pct_asian': 'pct asian (median)',\n    'pct_poverty': 'pct poverty (mean)',\n    'pct_comm_drive_alone': 'pct drive_alone (median)',\n    'pct_comm_carpool': 'pct carpool (median)',\n    'pct_comm_transit': 'pct transit (median)',\n    'pct_comm_walk': 'pct walk (median)',\n    'pct_comm_bike': 'pct bike (median)'\n}, inplace=True)\n\n# set display options\npd.set_option('display.precision', 2)\npd.set_option('display.colheader_justify', 'center')\n\n# display df as an html table\ndisplay(HTML(quintile_demographics.to_html(index=False)))\n\n\n\n\nquintile_road\nmedian income (mean)\npct white (median)\npct black (median)\npct hispanic (median)\npct asian (median)\npct poverty (mean)\npct drive_alone (median)\npct carpool (median)\npct transit (median)\npct walk (median)\npct bike (median)\n\n\n\n\n1\n71703.79\n59.82\n15.30\n5.35\n3.32\n11.99\n60.75\n6.77\n12.02\n1.01\n0.00\n\n\n2\n55323.21\n27.46\n28.12\n5.87\n2.53\n16.77\n52.47\n6.46\n14.20\n2.16\n0.00\n\n\n3\n59099.29\n25.49\n30.68\n8.17\n3.96\n22.49\n44.64\n6.98\n18.50\n2.52\n0.00\n\n\n4\n47287.19\n10.13\n56.15\n6.64\n3.24\n26.89\n43.46\n5.42\n23.37\n3.48\n0.00\n\n\n5\n49366.47\n22.62\n26.49\n5.40\n4.94\n26.36\n29.14\n2.61\n23.60\n6.43\n1.34\n\n\n\n\n\nTracts in lower quintiles of crashes per road length have higher median incomes, white populations, and rates of commuting to work in cars, while tracts in higher quintiles have lower median incomes, greater Black, Hispanic, and Asian populations, and higher rates of commuting to work by public transit, walking, and biking. This suggests that more marginalized communities bear a greater burden of non-motorist crash injuries.\nHowever, there is an important limitation to consider: the neighborhood characteristics of crash locations are not the same as those of the people injured in the crashes. This limitation is tempered by the fact that we are studying non-motorist crashes; in particular, injured non-motorists are more likely to be from nearby areas by virtue of their slower modes of transportation.",
    "crumbs": [
      "Project introduction",
      "Further data processing and viz."
    ]
  },
  {
    "objectID": "analysis/modeling.html",
    "href": "analysis/modeling.html",
    "title": "Building our random forest model",
    "section": "",
    "text": "Now that we have loaded and processed our data, we will proceed to build and test our random forest model.\n# install libraries if needed\n#%pip install statsmodels\n\n# load necessary libraries\nimport os\nimport requests\nimport pysal\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\nimport folium\nimport osmnx as ox\nimport statsmodels.api as sm\nfrom folium import plugins\nfrom sklearn.preprocessing import QuantileTransformer\nfrom shapely.geometry import Point, LineString\nfrom shapely.ops import nearest_points\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_poisson_deviance\n\nRequirement already satisfied: statsmodels in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (0.14.4)\nRequirement already satisfied: numpy&lt;3,&gt;=1.22.3 in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from statsmodels) (1.26.4)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.8 in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from statsmodels) (1.14.1)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from statsmodels) (1.5.3)\nRequirement already satisfied: patsy&gt;=0.5.6 in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from statsmodels) (0.5.6)\nRequirement already satisfied: packaging&gt;=21.3 in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from statsmodels) (24.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2024.2)\nRequirement already satisfied: six in /Users/songj/miniforge3/envs/musa-550-fall-2024/lib/python3.10/site-packages (from patsy&gt;=0.5.6-&gt;statsmodels) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nFirst, let’s load the data from the previous notebook.\n# load processed data\ngeo_data = gpd.read_file(\"geo_data_crashes.geojson\")",
    "crumbs": [
      "Project introduction",
      "Building our random forest model"
    ]
  },
  {
    "objectID": "analysis/modeling.html#model-design",
    "href": "analysis/modeling.html#model-design",
    "title": "Building our random forest model",
    "section": "Model design",
    "text": "Model design\nBecause the goal of this project is to predict future non-motorist crashes, we need to show how well our model can perform on existing data where we can verify predictions with observed data. Therefore, we will predict 2023 crashes using data from the prior five years - 2018 through 2022.\n\nPreparing the predictors\nAlong with our crash count columns, we select the variables we created that are associated with commuting patterns, vehicle ownership, and road characteristics.\n\n# select necessary columns\npredictor_columns = [col for col in geo_data.columns if col.startswith(\"crash_\") or col.endswith(\"_proportion\") or col == \"total_length\" or col == \"intersection_count\" or col.startswith(\"pct_comm\") or col.startswith(\"pct_veh\")]\n\n# ensure the 2023 crash variable is included\ntarget_column = \"crash_2023\"\n\n# filter the data to include only the selected columns and the target variable\ndata = geo_data[predictor_columns + [target_column]]\n\n# drop rows with missing values\ndata = data.dropna()\n\n# set columns for training and use data from 2018 through 2022\ntrain_columns = [col for col in predictor_columns if col.startswith(\"crash_2018\") or col.startswith(\"crash_2019\") or col.startswith(\"crash_2020\") or col.startswith(\"crash_2021\") or col.startswith(\"crash_2022\") or col.endswith(\"_proportion\") or col == \"total_length\" or col == \"intersection_count\" or col.startswith(\"pct_\")]\n\n# remove the \"pct_vehicle3plus\" column for model simplicity and robustness\ntrain_columns = [col for col in train_columns if col != \"pct_vehicle3plus\"]\n\n\nCorrelation matrix\nThe following matrix shows the correlations between the predictor variables. There is likely collinearity in the crash counts and lagged crashes, but we will keep them because we believe they are essential for the model.\n\n# filter data to include only selected columns\ncorr_data = geo_data[train_columns]\n\n# compute correlation matrix\ncorrelation_matrix = corr_data.corr()\n\n# visualize correlation matrix\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nModel setup\nNow, we will proceed to set up the random forest model. We will also add a robust scaler to the model instead of the standard scaler because it uses the median and interquartile range, which are less sensitive to outliers than the mean and standard deviation. We also use GridSearchCV to cross-validate the model and determine a set of optimized parameters.\n\n# set up training and testing datasets\nX_train = data[train_columns]\ny_train = data[target_column]\nX_test = data[train_columns]\ny_test = data[target_column]\n\n# deduplicate y columns\ny_train = y_train.loc[:, ~y_train.columns.duplicated()]\ny_test = y_test.loc[:, ~y_test.columns.duplicated()]\n\n# reshape y_train and y_test to be 1-dimensional arrays\ny_train = np.ravel(y_train)\ny_test = np.ravel(y_test)\n\n# set up random forest pipeline\nforest = make_pipeline(\n    RobustScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# generate hyperparameter grid\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [2, 5, 7, 9, 13, 21, 33, 51],\n}\n\n# create grid and use 3-fold CV\ngrid = GridSearchCV(forest, param_grid, cv=3, verbose=1)\n\n# run the model\ngrid.fit(X_train, y_train)\n\n# optional - display optimized hyperparameters\n#grid.best_params_\n\n# optional - show score\n#grid.score(X_test, y_test)\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('robustscaler', RobustScaler()),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('robustscaler', RobustScaler()),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('robustscaler', RobustScaler()),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])RobustScalerRobustScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\nNow, we will fit and evaluate the random forest model using two metrics: mean absolute error and Poisson deviance. The mean absolute error gives us the error in terms of crashes, while the Poisson deviance is a goodness-of-fit metric for that we calculate mainly to provide a standard of comparison to other models. A lower score indicates a better fit to the Poisson distribution, which is appropriate for count-based events like injuries and crashes.\n\n# use the optimal random forest model\nbest_random = grid.best_estimator_\n\n# make the predictions\npredictions = best_random.predict(X_test)\n\n# calculate absolute error\nerrors = predictions - y_test\nabs_errors = abs(predictions - y_test)\navg_error = np.mean(abs_errors)\n\n# calculate poisson deviance\npoisson_deviance = mean_poisson_deviance(y_test, predictions)\n\nprint(\"Model Performance\")\nprint(f\"Mean Absolute Error: {avg_error:0.4f} crashes\")\nprint(\"We do not report MAPE because of zero values in the observed data\")\nprint(f\"Poisson Deviance: {poisson_deviance}\")\n\nModel Performance\nAverage Absolute Error: 0.8172 crashes\nWe do not report MAPE because of zero values in the observed data\nPoisson Deviance: 0.5216537518327811\n\n\nThe model predictions had a mean absolute error of about 0.8 crashes, which is nowhere close to perfect, but may still be a decent model in comparison to other options. The Poisson deviance is approximately 0.52.\nNow, let’s map the predicted values.\n\n# merge predictions back into geo_data\ngeo_data['predicted_crashes'] = predictions\n\n# define a color map and normalization\nvmin = geo_data['predicted_crashes'].min()\nvmax = geo_data['predicted_crashes'].max()\nnorm = mcolors.Normalize(vmin=vmin, vmax=vmax)\ncmap = 'OrRd'  # Using 'OrRd' as the color map\n\n# create the figure and axis\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\n# plot the map for predicted crashes\ngeo_data.plot(column='predicted_crashes', cmap=cmap, norm=norm, ax=ax, legend=False)\nax.set_title('Predicted Crashes for 2023')\n\n# add a color bar\nscalarmap = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nscalarmap.set_array([])\ncbar = fig.colorbar(scalarmap, ax=ax, orientation='horizontal', fraction=0.02, pad=0.1)\ncbar.set_label('Predicted Crash Values')\n\n# remove the axes\nax.axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe model predicts high crash counts in University City, Kensington, and the Market East/Chinatown section of Center City, as well as along North Broad Street and the western end of Market Street. The large tract encompassing Fairmount Park and parts of West Philadelphia also has a higher number of crashes.\nLet’s now evaluate the model performance by mapping the errors.\n\n# add errors to geo_data\ngeo_data['rf_errors'] = errors\n\n# define color map and normalization for random forest errors\nvmin = geo_data['rf_errors'].min()\nvmax = geo_data['rf_errors'].max()\nnorm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\ncmap = 'coolwarm'  # Using 'coolwarm' as the diverging color map\n\n# create figure and axes\nfig, ax = plt.subplots(1, 1, figsize=(10, 7))\n\n# map for random forest errors\ngeo_data.plot(column='rf_errors', cmap=cmap, norm=norm, ax=ax, legend=False)\nax.set_title('Random Forest Errors')\n\n# color bar\nscalarmap = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nscalarmap.set_array([])\ncbar = fig.colorbar(scalarmap, ax=ax, orientation='horizontal', fraction=0.02, pad=0.1)\ncbar.set_label('Error Values')\n\n# remove the axes\nax.axis('off')\n\n# show map\nplt.show()\n\n\n\n\n\n\n\n\nThe map shows that error values are distributed across the city and not clustered in any specific area, other than University City, which has the greatest error magnitude at around -6 crashes. The map also shows that the model did not severely overpredict or underpredict in most tracts.",
    "crumbs": [
      "Project introduction",
      "Building our random forest model"
    ]
  },
  {
    "objectID": "analysis/modeling.html#comparison-with-poisson-regression",
    "href": "analysis/modeling.html#comparison-with-poisson-regression",
    "title": "Building our random forest model",
    "section": "Comparison with Poisson regression",
    "text": "Comparison with Poisson regression\nFor the sake of comparison with a more traditional model that is used for count data, we will run a Poisson regression to predict 2023 crash counts using the same predictor variables as the random forest model.\n\n# add a constant to the predictors (intercept term)\nX_train_nb = sm.add_constant(X_train)\nX_test_nb = sm.add_constant(X_test)\n\n# fit the poisson regression model\nnb_model = sm.GLM(y_train, X_train_nb, family=sm.families.Poisson()).fit()\n\n# make predictions\nnb_predictions = nb_model.predict(X_test_nb)\n\n# calculate the absolute errors\nnb_errors = abs(nb_predictions - y_test)\n\n# calculate average error\nnb_avg_error = np.mean(nb_errors)\n\n# calculate poisson deviance\nnb_poisson_deviance = mean_poisson_deviance(y_test, nb_predictions)\n\nprint(\"Poisson model performance\")\nprint(f\"Average absolute error: {nb_avg_error:0.4f} crashes\")\nprint(\"We do not report MAPE because of zero values in the observed data\")\nprint(f\"Poisson Deviance: {nb_poisson_deviance}\")\n\nPoisson model performance\nAverage absolute error: 1.5590 crashes\nWe do not report MAPE because of zero values in the observed data\nPoisson Deviance: 1.3527141666894478\n\n\nThe average absolute error was much greater than that of the random forest model, as well as the Poisson deviance. This suggests that the Poisson model underperforms in comparison to the random forest model.\n\n# add absolute errors to geo_data\ngeo_data['nb_errors'] = nb_errors\ngeo_data['abs_errors'] = abs_errors\n\n# define common color map and normalization\nvmin = min(geo_data['abs_errors'].min(), geo_data['nb_errors'].min())\nvmax = max(geo_data['abs_errors'].max(), geo_data['nb_errors'].max())\nnorm = mcolors.Normalize(vmin=vmin, vmax=vmax)\ncmap = 'OrRd'  # Using 'OrRd' as the common color map\n\n# create figure and axes\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n\n# map for random forest errors\ngeo_data.plot(column='abs_errors', cmap=cmap, norm=norm, ax=ax1, legend=False)\nax1.set_title('Random Forest Absolute Errors')\n\n# map for poisson errors\ngeo_data.plot(column='nb_errors', cmap=cmap, norm=norm, ax=ax2, legend=False)\nax2.set_title('Poisson Absolute Errors')\n\n# shared color bar\nscalarmap = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nscalarmap.set_array([])\ncbar = fig.colorbar(scalarmap, ax=[ax1, ax2], orientation='horizontal', fraction=0.02, pad=0.1)\ncbar.set_label('Error Values')\n\n# remove the axes\nax1.axis('off')\nax2.axis('off')\n\n# show maps\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the Poisson model had huge errors when predicting crashes in University City, as well as some tracts in Kensington and Center City. The random forest model also overpredicted in University City, but not nearly to the same extent.\nFrom this comparison, we suggest that predictions of non-motorist crash risk, as well as motor vehicle crash risk in general, can benefit from more advanced machine learning techniques like optimized random forest regression.\nNow, let’s produce predictions for 2024 before wrapping up our analysis.\n\n# set columns to predict on and use data from 2019 through 2023\ntrain_columns_24 = [col for col in predictor_columns if col.startswith(\"crash_2019\") or col.startswith(\"crash_2020\") or col.startswith(\"crash_2021\") or col.startswith(\"crash_2022\") or col.startswith(\"crash_2023\") or col.endswith(\"_proportion\") or col == \"total_length\" or col == \"intersection_count\" or col.startswith(\"pct_\")]\n\n# remove the \"pct_vehicle3plus\" column for model simplicity and robustness\ntrain_columns_24 = [col for col in train_columns if col != \"pct_vehicle3plus\"]\n\nX_test_24 = data[train_columns_24]\npredictions_24 = best_random.predict(X_test_24)\n\n# merge predictions back into geo_data\ngeo_data['predicted_24'] = predictions_24\n\n# define a color map and normalization\nvmin = geo_data['predicted_24'].min()\nvmax = geo_data['predicted_24'].max()\nnorm = mcolors.Normalize(vmin=vmin, vmax=vmax)\ncmap = 'OrRd'  # Using 'OrRd' as the color map\n\n# create the figure and axis\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\n# plot the map for predicted crashes\ngeo_data.plot(column='predicted_24', cmap=cmap, norm=norm, ax=ax, legend=False)\nax.set_title('Predicted Crashes for 2024')\n\n# add a color bar\nscalarmap = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nscalarmap.set_array([])\ncbar = fig.colorbar(scalarmap, ax=ax, orientation='horizontal', fraction=0.02, pad=0.1)\ncbar.set_label('Predicted Crash Values')\n\n# remove the axes\nax.axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe predictions are quite similar between 2023 and 2024, which is expected because of the four shared years of data between the two predictor datasets.",
    "crumbs": [
      "Project introduction",
      "Building our random forest model"
    ]
  },
  {
    "objectID": "analysis/modeling.html#conclusion",
    "href": "analysis/modeling.html#conclusion",
    "title": "Building our random forest model",
    "section": "Conclusion",
    "text": "Conclusion\nBased on our results, we suggest that random forest modeling techniques are useful for predicting non-motorist crash risk in Philadelphia. Our prediction of non-motorist crash risk shows that University City, Market East, Fairmount Park, North Broad Street, western Market Street, and Kensington should be focus areas for traffic calming and urban design improvements to prevent further vehicular collisions with non-motorists.\nWe also recommend further refinement of the model, using more relevant data and adjusting the number of years of prior crash data to use in the prediction process. The geographic unit may be changed to block groups, fishnet grid cells, or road segments to improve the granularity and precision of the features and model predictions. Further variables to describe the characteristics of the road network, such as speed limits and highway off-ramp locations, may be useful in improving the accuracy of the model.",
    "crumbs": [
      "Project introduction",
      "Building our random forest model"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Project introduction",
    "section": "",
    "text": "Cars dominate daily life in the United States. Most Americans depend on driving to get to work, school, and other everyday activities as the most time-efficient form of transportation available to them. However, motor vehicles have faced criticism as air polluters, contributors to greenhouse gas emissions, and dangers not only to the people in them, but also to the non-motorists sharing the road. In response to the latter concerns, Mayor Jim Kenney signed an executive order in 2016 to undertake Vision Zero, a strategy to eliminate all traffic-related deaths by 2030. (City of Philadelphia)\nIn the past several years, traffic injuries have decreased from a high of over 3 million in 2016 to 2.38 million in 2022. However, traffic fatalities experienced a surge following the onset of the COVID-19 pandemic. (NHTSA) Philadelphia was not an exception to this national trend: traffic fatalities rose from 84 in 2019 to 152 in 2020. (Axios)\n\n\nModern urbanism has focused on human-centered approaches to improving urban life. These include developing walkable and bikeable neighborhoods, which provide health benefits to its residents through incorporating regular exercise into daily routines, as well as reducing vehicular pollution and injury risk. Urban planners sometimes cite Dutch cities like Amsterdam and Utrecht as prime examples of this urbanist ideal, where bicycles and light rail tend to dominate the streets instead of cars. The streets of US cities, on the other hand, are almost all dominated by cars, which is laid bare in their wide streets, car-prioritized intersections, and high speed limits even within densely populated areas.\nWith dangerous streets and high motor vehicle usage rates in the US, the persistence of motor vehicle crash injuries is not surprising. Non-motorists are of special concern - not only because they are more vulnerable to severe injury when colliding with a large motor vehicle, but also because they tend to represent people who cannot afford a car or rideshare. As we will show in our analysis, the areas affected most by non-motorist crashes are disproportionately Black and brown, with lower incomes and higher poverty rates. Injuries to non-motorists also bring up a moral hazard: those who cannot or choose not to use a motor vehicle are still being injured and killed by them. Despite these concerns, non-motorist crashes are understudied in injury epidemiology. This project will aim to address a knowledge gap regarding non-motorist crashes.\n\n\n\n\nTo ensure that we continue to reduce crashes and traffic injuries despite a limited city budget, we must undertake strategic interventions and investments in the areas where crash risk is highest. Prior data show us where crashes have already occurred, but without further modeling, it may be difficult to accurately predict where crashes may occur in the future. Therefore, in this project, we will demonstrate how machine learning can be used to predict non-motorist crash risk in Philadelphia.",
    "crumbs": [
      "Project introduction"
    ]
  },
  {
    "objectID": "analysis/index.html#motor-vehicle-injuries-in-philadelphia",
    "href": "analysis/index.html#motor-vehicle-injuries-in-philadelphia",
    "title": "Project introduction",
    "section": "",
    "text": "Cars dominate daily life in the United States. Most Americans depend on driving to get to work, school, and other everyday activities as the most time-efficient form of transportation available to them. However, motor vehicles have faced criticism as air polluters, contributors to greenhouse gas emissions, and dangers not only to the people in them, but also to the non-motorists sharing the road. In response to the latter concerns, Mayor Jim Kenney signed an executive order in 2016 to undertake Vision Zero, a strategy to eliminate all traffic-related deaths by 2030. (City of Philadelphia)\nIn the past several years, traffic injuries have decreased from a high of over 3 million in 2016 to 2.38 million in 2022. However, traffic fatalities experienced a surge following the onset of the COVID-19 pandemic. (NHTSA) Philadelphia was not an exception to this national trend: traffic fatalities rose from 84 in 2019 to 152 in 2020. (Axios)\n\n\nModern urbanism has focused on human-centered approaches to improving urban life. These include developing walkable and bikeable neighborhoods, which provide health benefits to its residents through incorporating regular exercise into daily routines, as well as reducing vehicular pollution and injury risk. Urban planners sometimes cite Dutch cities like Amsterdam and Utrecht as prime examples of this urbanist ideal, where bicycles and light rail tend to dominate the streets instead of cars. The streets of US cities, on the other hand, are almost all dominated by cars, which is laid bare in their wide streets, car-prioritized intersections, and high speed limits even within densely populated areas.\nWith dangerous streets and high motor vehicle usage rates in the US, the persistence of motor vehicle crash injuries is not surprising. Non-motorists are of special concern - not only because they are more vulnerable to severe injury when colliding with a large motor vehicle, but also because they tend to represent people who cannot afford a car or rideshare. As we will show in our analysis, the areas affected most by non-motorist crashes are disproportionately Black and brown, with lower incomes and higher poverty rates. Injuries to non-motorists also bring up a moral hazard: those who cannot or choose not to use a motor vehicle are still being injured and killed by them. Despite these concerns, non-motorist crashes are understudied in injury epidemiology. This project will aim to address a knowledge gap regarding non-motorist crashes.",
    "crumbs": [
      "Project introduction"
    ]
  },
  {
    "objectID": "analysis/index.html#modeling-to-inform-strategic-interventions",
    "href": "analysis/index.html#modeling-to-inform-strategic-interventions",
    "title": "Project introduction",
    "section": "",
    "text": "To ensure that we continue to reduce crashes and traffic injuries despite a limited city budget, we must undertake strategic interventions and investments in the areas where crash risk is highest. Prior data show us where crashes have already occurred, but without further modeling, it may be difficult to accurately predict where crashes may occur in the future. Therefore, in this project, we will demonstrate how machine learning can be used to predict non-motorist crash risk in Philadelphia.",
    "crumbs": [
      "Project introduction"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Non-motorist crash risk in Philadelphia",
    "section": "",
    "text": "Or, use this link to start at the beginning of the project.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome-to-my-musa-550-final-project",
    "href": "index.html#welcome-to-my-musa-550-final-project",
    "title": "Non-motorist crash risk in Philadelphia",
    "section": "",
    "text": "Or, use this link to start at the beginning of the project.",
    "crumbs": [
      "Welcome"
    ]
  }
]